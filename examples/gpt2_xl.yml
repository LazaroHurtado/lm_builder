attention: CausalMultiHeadAttention
attention_config:
  context_length: 1024
  embedding_dimension: 1600
  num_heads: 25
  attn_dropout: 0.1
  resid_dropout: 0.1
  with_kv_cache: True
  bias: True
ffn: ClassicFeedForward
ffn_config:
  embedding_dimension: 1600
  intermediate_dimension: 6400
  dropout: 0.1
  activation_fn: GELU
  bias: True
positional_embedding: AbsolutePE
dropout: 0.1
vocab_size: 50257
num_layers: 48
bias: True
norm_bias: True