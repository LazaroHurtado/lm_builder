attention: GroupedQueryAttention
attention_config:
  positional_embedding: RotaryPE
  context_length: 2048
  embedding_dimension: 4096
  num_heads: 32
  kv_heads: 32
ffn: FeedForward
ffn_config:
  embedding_dimension: 4096
  intermediate_dimension: 11008
  activation_fn: SiLU
token_embedding: Embedding
vocab_size: 32000
num_layers: 32