attention: GroupedQueryAttention
attention_config:
  positional_embedding: RotaryPE
  context_length: 512
  embedding_dimension: 4096
  num_heads: 32
  kv_heads: 32
ffn: FeedForward
ffn_config:
  embedding_dimension: 4096
  intermediate_dimension: 11008
  activation_fn: SiLU
norm: RMSNorm
ffn_norm: RMSNorm
attn_norm: RMSNorm
vocab_size: 32000
num_layers: 32