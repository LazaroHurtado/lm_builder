attention: GroupedQueryAttention
attention_config:
  positional_embedding: RotaryPE
  context_length: 1024
  embedding_dimension: 4096
  num_heads: 32
  kv_heads: 32
  with_kv_cache: True
ffn: FeedForward
ffn_config:
  embedding_dimension: 4096
  intermediate_dimension: 11008
  activation_fn: SiLU
vocab_size: 32000
num_layers: 32